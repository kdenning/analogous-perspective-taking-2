---
title: "Analogous Experiment 2 - Results"
output: 
    html_document:
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 1
      code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Loading packages
library(psych)
library(lme4)
library(nlme)
library(sjPlot)
library(effects)
library(magrittr) # part of the tidyverse but must be read in on its own
library(parameters)
library(dplyr)
library(tidyr)
library(rio)
library(ggplot2)

# Functions to clean document, get data from wide to long format
source("functions/Cleaning.R")
source("functions/repeat_funcs.R")

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

options(scipen = 999)

# Importing data
wide_raw <- import("data/analog2_wide_pt to remove.csv") 

# Cleaning data using functions
wide_data_clean <- wide_factor_clean(wide_raw)

long_data <- wide_to_long(wide_data_clean)
```

# Descriptives {.tabset .tabset-fade .tabset-pills}

## Overall descriptives

```{r}
psych::describe(long_data)
```

## Histograms

### Confirmatory

```{r}
hist(long_data$ingroup_ident)

hist(long_data$threat)
````

Identification is roughly normal, while threat is closer to flat.

## Correlations

## Boxplots

```{r}
box_ident_res <- boxplot(long_data$ingroup_ident)

boxplot(long_data$threat)

range(box_ident_res$out)

sd <- round(sd(na.omit(long_data$ingroup_ident)),2)
mean <- round(mean(na.omit(long_data$ingroup_ident)),2)
upper_cut <- mean + 3*sd
lower_cut <- mean - 3*sd

long_data %>% 
  select(sub_id, ingroup_ident) %>% 
  unique() %>% 
  filter(ingroup_ident < lower_cut | ingroup_ident > upper_cut)
```

The outliers are not over our criteria of 3 SD so they will not be removed. Plus, they are theoretically important as the weakly identified individuals.

```{r}
boxplot(long_data$target_bfi_value ~ long_data$covid1) 

boxplot(long_data$target_bfi_value ~ long_data$covid2)

boxplot(long_data$target_bfi_value ~ long_data$covid3)
```

# Demographics {.tabset .tabset-fade .tabset-pills}

## Sample Size

### Overall:

```{r n}
long_data %>% 
  select(sub_id) %>% 
  unique() %>% 
  nrow()
```

### N per Voting Opinion

```{r n by group and voting opin}
long_data %>% 
  select(sub_id, cand_pref) %>% 
  unique() %>% 
  group_by(cand_pref) %>% 
  count()
```

I'm wondering with so few Trump supporters if we should remove them from the analysis... There isn't enough for a comparison condition

## Gender

```{r gender}
long_data %>% 
  na.omit() %>% 
  select(sub_id, gender) %>% 
  unique() %>% 
  group_by(gender) %>% 
  count()
```

## Race/Ethnicity

```{r race}
ethnicity_counts <- long_data %>% 
  select(sub_id, ethnicity) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(ethnicity) %>% 
  count()

ethnicity_counts %>% 
  mutate(n_total = sum(ethnicity_counts$n),
    percent = n/n_total*100) %>% 
  select(-n_total)
```

## Age

```{r age}
long_data %>% 
  select(sub_id, age) %>% 
  na.omit() %>% 
  summarize(mean = mean(age),
            sd = sd(age))
```

## COVID Questions

### Personal experience with COVID

```{r}
long_data %>% 
  select(sub_id, covid1) %>% 
  unique() %>% 
  group_by(covid1) %>% 
  count()
```

### How many people they see

```{r}
long_data %>% 
  select(sub_id, covid2) %>% 
  unique() %>% 
  group_by(covid2) %>% 
  count()
```

### Most important issue related to COVID

```{r}
long_data %>% 
  select(sub_id, covid3) %>% 
  unique() %>% 
  group_by(covid3) %>% 
  count()
```

# Basic Model

```{r}
# Listwise removal of missing data per item
model_data <- long_data %>% 
  select(sub_id, bfi_number, cand_pref, ingroup_ident, threat, 
         self, target_number_collapsed, target_bfi_value, 
         condition_order, covid1, covid2, covid3, target_group,
         ingroup_ident_c, self_c, threat_c) %>% 
  na.omit(long_data)

```

## Contrasts

### Target number

```{r contrasts 1}
## Dummy coding with first target as reference as wanted
contrasts(model_data$target_number_collapsed) 
```

### Target group

```{r contrasts 2}
## Levels for target group were opposite, so switched them and applied effects coding
## for between-subjects differences of target (which is based on political preference
## - it is the opposite of the target's preference)
levels(model_data$target_group)
model_data$target_group <- relevel(model_data$target_group, "Trump Supporter Target")
contrasts(model_data$target_group) <- contr.sum(2)
```

```{r}
# Model with self items as RE also did not converge
# For reference about nesting: https://www.muscardinus.be/2017/07/lme4-random-effects/
model_base <- lmer(target_bfi_value ~ self_c*target_number_collapsed + (self_c|target_number_collapsed), data = model_data)

# lm.form <- formula(target_bfi_value ~ 1 + self_c + target_number_collapsed + 
#                      ingroup_ident_c + threat_c + self_c:target_number_collapsed + 
#                      self_c:ingroup_ident_c + self_c:threat_c + 
#                      self_c:target_number_collapsed:ingroup_ident_c + 
#                      self_c:target_number_collapsed:threat_c)
```

## Residuals & Heteroscedasticity

```{r}
lm.form <- formula(target_bfi_value ~ 1 + self_c + target_number_collapsed + self_c:target_number_collapsed)

control1 <- lmeControl(maxIter = 10000, msMaxIter = 10000, niterEM = 10000,
                       msMaxEval = 10000, opt = c("nlminb"), optimMethod = "BFGS", returnObject=TRUE)

model_base_nlme1 <- lme(lm.form,
                        random = ~1 + self_c | sub_id,
                        data = model_data,
                        na.action = na.exclude,
                        varIdent(form = ~1 | sub_id),
                        control = control1)

# lmeControl(opt = "optim") # alternative code for control above; doesn't have convergence issue but has allocation issue
model_base_nlme2 <- lme(lm.form,
                        random = ~1 + self_c | sub_id,
                        data = model_data,
                        na.action = na.exclude)
```




## Results

```{r}
tab_model(model_base,
          digits = 3)
```

Nothing is significant

## Plot

```{r}
mod1_effects <- get_me_effects(model_base, 
                               "self_c:target_number_collapsed", 
                               target_number_collapsed,
                               "target_1",
                               "target_2")
#Using model for paremeters
ggplot(mod1_effects, aes(self_c, fit, group = target_number_collapsed)) +
    geom_smooth(method = "lm", 
                size = .7, 
                colour = "black", 
                aes(linetype = target_number_collapsed)) +
    theme_minimal(base_size = 13) +
    theme(legend.key.size = unit(1, "cm"))

# Without using model to get se bars
ggplot(model_data, aes(self_c, target_bfi_value, group = target_number_collapsed)) +
    geom_smooth(method = "lm", 
                size = .7, 
                colour = "black", 
                aes(linetype = target_number_collapsed)) +
    theme_minimal(base_size = 13) +
    theme(legend.key.size = unit(1, "cm"))

```

I don't get why it isn't significant... Do we not have enough within-subjects time points to do random effects? When it isn't random effects, it is signfiicnat (I did this first when I was getting a convergence issue due to misassigning the model). I think I have the model correct now but nothing is significant (its the opposite). I didn't think the effect would be this drastic with within-subjects analyses.
