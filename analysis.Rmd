---
title: "Analogous Experiment 2 - Results"
output: 
    html_document:
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 1
      code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Loading packages
library(psych)
library(lme4)
library(nlme)
library(sjPlot)
library(effects)
library(magrittr) # part of the tidyverse but must be read in on its own
library(parameters)
library(dplyr)
library(tidyr)
library(rio)
library(ggplot2)

# Functions to clean document, get data from wide to long format
source("functions/Cleaning.R")
source("functions/repeat_funcs.R")

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

options(scipen = 999)

# Importing data
wide_raw <- import("data/analog2_wide_pt to remove.csv") 

# Cleaning data using functions
wide_data_clean <- wide_factor_clean(wide_raw)

long_data <- wide_to_long(wide_data_clean)
```

# Descriptives {.tabset .tabset-fade .tabset-pills}

## Overall descriptives

```{r}
psych::describe(long_data)
```

## Histograms

### Confirmatory

```{r}
hist(long_data$ingroup_ident)

hist(long_data$threat)
````

Identification is roughly normal, while threat is closer to flat.

## Correlations

## Boxplots

```{r}
box_ident_res <- boxplot(long_data$ingroup_ident)

boxplot(long_data$threat)

range(box_ident_res$out)

sd <- round(sd(na.omit(long_data$ingroup_ident)),2)
mean <- round(mean(na.omit(long_data$ingroup_ident)),2)
upper_cut <- mean + 3*sd
lower_cut <- mean - 3*sd

long_data %>% 
  select(sub_id, ingroup_ident) %>% 
  unique() %>% 
  filter(ingroup_ident < lower_cut | ingroup_ident > upper_cut)
```

The outliers are not over our criteria of 3 SD so they will not be removed. Plus, they are theoretically important as the weakly identified individuals.

```{r}
boxplot(long_data$target_bfi_value ~ long_data$covid1) 

boxplot(long_data$target_bfi_value ~ long_data$covid2)

boxplot(long_data$target_bfi_value ~ long_data$covid3)
```

# Demographics {.tabset .tabset-fade .tabset-pills}

## Sample Size

### Overall:

```{r n}
long_data %>% 
  select(sub_id) %>% 
  unique() %>% 
  nrow()
```

### N per Voting Opinion

```{r n by group and voting opin}
long_data %>% 
  select(sub_id, cand_pref) %>% 
  unique() %>% 
  group_by(cand_pref) %>% 
  count()
```

I'm wondering with so few Trump supporters if we should remove them from the analysis... There isn't enough for a comparison condition

## Gender

```{r gender}
long_data %>% 
  na.omit() %>% 
  select(sub_id, gender) %>% 
  unique() %>% 
  group_by(gender) %>% 
  count()
```

## Race/Ethnicity

```{r race}
ethnicity_counts <- long_data %>% 
  select(sub_id, ethnicity) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(ethnicity) %>% 
  count()

ethnicity_counts %>% 
  mutate(n_total = sum(ethnicity_counts$n),
    percent = n/n_total*100) %>% 
  select(-n_total)
```

## Age

```{r age}
long_data %>% 
  select(sub_id, age) %>% 
  na.omit() %>% 
  summarize(mean = mean(age),
            sd = sd(age))
```

## COVID Questions

### Personal experience with COVID

```{r}
long_data %>% 
  select(sub_id, covid1) %>% 
  unique() %>% 
  group_by(covid1) %>% 
  count()
```

### How many people they see

```{r}
long_data %>% 
  select(sub_id, covid2) %>% 
  unique() %>% 
  group_by(covid2) %>% 
  count()
```

### Most important issue related to COVID

```{r}
long_data %>% 
  select(sub_id, covid3) %>% 
  unique() %>% 
  group_by(covid3) %>% 
  count()
```

# Basic Model

```{r}
# Listwise removal of missing data per item
model_data <- long_data %>% 
  select(sub_id, bfi_number, cand_pref, ingroup_ident, threat, 
         self, target_number_collapsed, target_bfi_value, 
         condition_order, covid1, covid2, covid3, target_group,
         ingroup_ident_c, self_c, threat_c) %>% 
  na.omit(long_data)

```

## Contrasts

### Target number

```{r contrasts 1}
## Dummy coding with first target as reference as wanted
contrasts(model_data$target_number_collapsed) 
```

### Target group

```{r contrasts 2}
## Levels for target group were opposite, so switched them and applied effects coding
## for between-subjects differences of target (which is based on political preference
## - it is the opposite of the target's preference)
levels(model_data$target_group)
model_data$target_group <- relevel(model_data$target_group, "Trump Supporter Target")
contrasts(model_data$target_group) <- contr.sum(2)
```

```{r}
# Model with self items as RE also did not converge
# For reference about nesting: https://www.muscardinus.be/2017/07/lme4-random-effects/
model_base <- lmer(target_bfi_value ~ self_c*target_number_collapsed + (self_c + target_number_collapsed|sub_id), data = model_data)
```

## Residuals & Heteroscedasticity

### Level 1 

```{r}
lm.form <- formula(target_bfi_value ~ 1 + self_c + target_number_collapsed + self_c:target_number_collapsed)

control1 <- lmeControl(maxIter = 10000, msMaxIter = 10000, niterEM = 10000,
                       msMaxEval = 10000, opt = c("nlminb"), optimMethod = "BFGS", returnObject=TRUE)

model_base_nlme1 <- lme(lm.form,
                        random = ~1 + self_c | sub_id,
                        data = model_data,
                        na.action = na.exclude,
                        varIdent(form = ~1 | sub_id),
                        control = control1)

# lmeControl(opt = "optim") # alternative code for control above; doesn't have convergence issue but has allocation issue
model_base_nlme2 <- lme(lm.form,
                        random = ~1 + self_c | sub_id,
                        data = model_data,
                        na.action = na.exclude)
```

### Level 2

#### Testing target_number_collapsed

```{r}
mod_vartest_targ <- lmer(target_bfi_value ~ self_c*target_number_collapsed + (self_c|sub_id), data = model_data)

anova(model_base, mod_vartest_targ)
```

The models in which target_number_collapsed are allowed to vary and are held to 0 differ significantly. Indicates the need to include the variances (and covariances) in the model.

```{r}
hist(ranef(model_base)$sub_id$`(Intercept)`, prob = T)
```

Normally distributed

```{r}
hist(ranef(model_base)$sub_id$target_number_collapsed, prob = T)
```

Normally distributed

```{r}
qqnorm(ranef(model_base)$sub_id$`(Intercept)`, 
       main="Q-Q Plot for L2 Residuals: Intercept")
qqline(ranef(model_base)$sub_id$`(Intercept)`, col = "steelblue", lwd = 2)
```

The residuals for the intercept look normal to me

```{r}
qqnorm(ranef(model_base)$sub_id$target_number_collapsed, 
       main="Q-Q Plot for L2 Residuals: Target Number")
qqline(ranef(model_base)$sub_id$target_number_collapsed, col = "steelblue", lwd = 2)
```

The residuals for target number look a little off, like they are shifting from normal to heavy tailed, which makes sense given the nature of the two groups. I'm not sure this is cause for concern given the data.


Will want to do this for the moderators as well. However, I want to make sure I can get the level 1 test to work.

#### Testing moderators

##### In-group identification

```{r}
mod_vartest_ident1 <- lmer(target_bfi_value ~ self_c*target_number_collapsed*ingroup_ident_c + (self_c + target_number_collapsed |sub_id), data = model_data)

# Model with ingroup_ident as moderator does not converge, as expected
# mod_vartest_ident2 <- lmer(target_bfi_value ~ self_c*target_number_collapsed*ingroup_ident + (self_c + target_number_collapsed + ingroup_ident |sub_id), data = model_data)

anova(mod_vartest_targ, mod_vartest_ident1)
```

There is a significant difference adding ingroup_ident into the model when doing model comparisons. Checking next model with threat

##### Threat

```{r}
mod_vartest_threat1 <- lmer(target_bfi_value ~ self_c*target_number_collapsed*ingroup_ident_c*threat_c + (self_c + target_number_collapsed |sub_id), data = model_data)

# Model with ingroup_ident as moderator does not converge, as expected
# mod_vartest_threat2 <- lmer(target_bfi_value ~ self_c*target_number_collapsed*ingroup_ident_c*threat_c + (self_c + target_number_collapsed + threat_c |sub_id), data = model_data)

anova(mod_vartest_ident1, mod_vartest_threat1)
```

And there is a significant difference between these models.

##### Using the threat model to look at Level 2 residuals affected by moderators


```{r}
hist(ranef(mod_vartest_threat1)$sub_id$`(Intercept)`, prob = T)
```

```{r}
hist(ranef(mod_vartest_threat1)$sub_id$target_number_collapsed, prob = T)
```

```{r}
qqnorm(ranef(mod_vartest_threat1)$sub_id$`(Intercept)`, 
       main="Q-Q Plot for L2 Residuals: Intercept")
qqline(ranef(mod_vartest_threat1)$sub_id$`(Intercept)`, col = "steelblue", lwd = 2)
```

```{r}
qqnorm(ranef(mod_vartest_threat1)$sub_id$target_number_collapsed, 
       main="Q-Q Plot for L2 Residuals: Target Number")
qqline(ranef(mod_vartest_threat1)$sub_id$target_number_collapsed, col = "steelblue", lwd = 2)
```

Everything looks roughly the same as the base model. As the moderators were not used as random effects, they cannot be used for these visualizations.

## Results

```{r}
tab_model(mod_vartest_threat1,
          digits = 3)
```

Main effect of target and main effect of threat, no higher-order interactions with the moderators.

## Plot

```{r}
ggplot(model_data, aes(self_c, target_bfi_value, group = target_number_collapsed)) +
    geom_smooth(method = "lm", 
                size = .7, 
                colour = "black", 
                aes(linetype = target_number_collapsed)) +
    theme_minimal(base_size = 13) +
    theme(legend.key.size = unit(1, "cm"))
```

